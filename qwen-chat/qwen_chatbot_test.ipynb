{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen1.5-0.5B-Chat Test Notebook\n",
    "\n",
    "基于香橙派AIpro + MindSpore 实现的通义千问Qwen聊天机器人测试笔记本\n",
    "\n",
    "**环境要求:**\n",
    "- CANN 8.1.RC1\n",
    "- MindSpore 2.6.0  \n",
    "- mindnlp 0.4.1\n",
    "\n",
    "**参考:** [昇腾社区 - 基于香橙派AIpro+MindSpore实现Qwen聊天机器人](https://www.hiascend.com/developer/techArticles/20250424-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境检查\n",
    "\n",
    "首先检查系统环境是否正确配置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check MindSpore version\n",
    "import mindspore\n",
    "print(f\"MindSpore version: {mindspore.__version__}\")\n",
    "\n",
    "# Check mindnlp version\n",
    "import mindnlp\n",
    "print(f\"mindnlp version: {mindnlp.__version__}\")\n",
    "\n",
    "# Check NPU availability\n",
    "from mindspore import context\n",
    "print(f\"MindSpore device target: {context.get_context('device_target')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available memory\n",
    "import psutil\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"Total Memory: {mem.total / (1024**3):.2f} GB\")\n",
    "print(f\"Available Memory: {mem.available / (1024**3):.2f} GB\")\n",
    "print(f\"Memory Used: {mem.percent}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 模型配置\n",
    "\n",
    "设置模型名称和数据类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
    "MS_DTYPE = mindspore.float16  # Use FP16 for memory efficiency\n",
    "\n",
    "# Generation parameters\n",
    "MAX_NEW_TOKENS = 1024\n",
    "TEMPERATURE = 0.1\n",
    "TOP_P = 0.9\n",
    "DO_SAMPLE = True\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Data type: {MS_DTYPE}\")\n",
    "print(f\"Max new tokens: {MAX_NEW_TOKENS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 加载模型和分词器\n",
    "\n",
    "**注意:** 首次运行需要从 Hugging Face 下载模型 (约 1GB)，可能需要几分钟时间。\n",
    "\n",
    "如果下载速度较慢，可以设置镜像源:\n",
    "```bash\n",
    "export HF_ENDPOINT=https://hf-mirror.com\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f\"Loading tokenizer for {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, ms_dtype=MS_DTYPE)\n",
    "print(\"Tokenizer loaded successfully!\")\n",
    "\n",
    "print(f\"\\nLoading model {MODEL_NAME}...\")\n",
    "print(\"(This may take 1-2 minutes on first run)\")\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, ms_dtype=MS_DTYPE)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model architecture\n",
    "print(f\"Model type: {type(model).__name__}\")\n",
    "print(f\"Model config:\\n{model.config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 定义聊天函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "# System prompt - defines the bot's personality\n",
    "SYSTEM_PROMPT = \"You are a helpful and friendly chatbot\"\n",
    "\n",
    "def build_input_from_chat_history(chat_history, msg: str):\n",
    "    \"\"\"Build message list from chat history and new message.\"\"\"\n",
    "    messages = [{'role': 'system', 'content': SYSTEM_PROMPT}]\n",
    "    for user_msg, ai_msg in chat_history:\n",
    "        messages.append({'role': 'user', 'content': user_msg})\n",
    "        messages.append({'role': 'assistant', 'content': ai_msg})\n",
    "    messages.append({'role': 'user', 'content': msg})\n",
    "    return messages\n",
    "\n",
    "def generate_response(message, chat_history=None, stream=False):\n",
    "    \"\"\"Generate a response from the model.\n",
    "    \n",
    "    Args:\n",
    "        message: User's input message\n",
    "        chat_history: List of (user_msg, ai_msg) tuples\n",
    "        stream: If True, yield partial responses as they're generated\n",
    "    \n",
    "    Returns:\n",
    "        If stream=True: Generator yielding partial responses\n",
    "        If stream=False: Complete response string\n",
    "    \"\"\"\n",
    "    if chat_history is None:\n",
    "        chat_history = []\n",
    "    \n",
    "    # Format messages for the model\n",
    "    messages = build_input_from_chat_history(chat_history, message)\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"ms\",\n",
    "        tokenize=True\n",
    "    )\n",
    "    \n",
    "    if stream:\n",
    "        # Streaming mode - yield partial responses\n",
    "        streamer = TextIteratorStreamer(\n",
    "            tokenizer, \n",
    "            timeout=300, \n",
    "            skip_prompt=True, \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        generate_kwargs = dict(\n",
    "            input_ids=input_ids,\n",
    "            streamer=streamer,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=DO_SAMPLE,\n",
    "            top_p=TOP_P,\n",
    "            temperature=TEMPERATURE,\n",
    "            num_beams=1,\n",
    "        )\n",
    "        t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "        t.start()\n",
    "        \n",
    "        partial_message = \"\"\n",
    "        for new_token in streamer:\n",
    "            partial_message += new_token\n",
    "            if '</s>' in partial_message:\n",
    "                break\n",
    "            yield partial_message\n",
    "    else:\n",
    "        # Non-streaming mode - return complete response\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=DO_SAMPLE,\n",
    "            top_p=TOP_P,\n",
    "            temperature=TEMPERATURE,\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        return response\n",
    "\n",
    "print(\"Chat functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 简单测试 - 无上下文\n",
    "\n",
    "测试模型对单个问题的响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test - single question, no context\n",
    "test_question = \"你好，请介绍一下你自己。\"\n",
    "\n",
    "print(f\"Question: {test_question}\")\n",
    "print(\"\\nGenerating response...\")\n",
    "\n",
    "response = generate_response(test_question)\n",
    "\n",
    "print(f\"\\nResponse: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 流式输出测试\n",
    "\n",
    "测试流式输出 - 逐token显示生成过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming test\n",
    "test_question = \"用Python写一个计算斐波那契数列的函数\"\n",
    "\n",
    "print(f\"Question: {test_question}\")\n",
    "print(\"\\nResponse (streaming):\\n\")\n",
    "\n",
    "full_response = \"\"\n",
    "for partial in generate_response(test_question, stream=True):\n",
    "    # Clear the line and print the new partial response\n",
    "    print(f\"\\r{partial}\", end=\"\", flush=True)\n",
    "    full_response = partial\n",
    "\n",
    "print(f\"\\n\\nFinal response:\\n{full_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 多轮对话测试\n",
    "\n",
    "测试多轮对话能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-turn conversation test\n",
    "chat_history = []\n",
    "\n",
    "def chat_turn(user_message, history):\n",
    "    \"\"\"Run one turn of the conversation.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"User: {user_message}\")\n",
    "    \n",
    "    response = generate_response(user_message, history)\n",
    "    \n",
    "    print(f\"\\nAssistant: {response}\")\n",
    "    \n",
    "    # Update history\n",
    "    history.append((user_message, response))\n",
    "    return history\n",
    "\n",
    "# Conversation turns\n",
    "chat_history = chat_turn(\"你好，我叫小明。\", chat_history)\n",
    "chat_history = chat_turn(\"我叫什么名字？\", chat_history)\n",
    "chat_history = chat_turn(\"你能帮我做什么？\", chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 代码生成测试\n",
    "\n",
    "测试模型的代码生成能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code generation test\n",
    "code_questions = [\n",
    "    \"用Python写一个冒泡排序函数\",\n",
    "    \"解释一下什么是递归，并给出一个例子\",\n",
    "    \"如何用Python读取CSV文件？\"\n",
    "]\n",
    "\n",
    "for question in code_questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"\\nResponse:\")\n",
    "    \n",
    "    response = generate_response(question)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 不同温度参数测试\n",
    "\n",
    "测试不同温度参数对生成结果的影响。\n",
    "\n",
    "- **Temperature = 0.1**: 更确定性的输出\n",
    "- **Temperature = 0.7**: 平衡的创造性和一致性\n",
    "- **Temperature = 1.0**: 更随机、更有创造性的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature parameter test\n",
    "test_prompt = \"写一个关于AI的小故事\"\n",
    "temperatures = [0.1, 0.7, 1.0]\n",
    "\n",
    "for temp in temperatures:\n",
    "    # Update global temperature\n",
    "    globals()['TEMPERATURE'] = temp\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Temperature = {temp}\")\n",
    "    print(f\"Prompt: {test_prompt}\")\n",
    "    print(\"\\nResponse:\")\n",
    "    \n",
    "    response = generate_response(test_prompt)\n",
    "    print(response)\n",
    "\n",
    "# Reset temperature to default\n",
    "TEMPERATURE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 交互式聊天测试\n",
    "\n",
    "在下面的单元格中输入你自己的问题进行测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive chat - modify the question and run this cell\n",
    "user_question = \"\"\"  # <- 在这里输入你的问题\n",
    "\"\"\"\n",
    "\n",
    "if user_question.strip():\n",
    "    print(f\"Question: {user_question}\")\n",
    "    print(\"\\nResponse:\")\n",
    "    response = generate_response(user_question)\n",
    "    print(response)\n",
    "else:\n",
    "    print(\"Please enter a question in the user_question variable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 性能测试\n",
    "\n",
    "测量模型推理的响应时间和内存使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "\n",
    "# Performance test\n",
    "test_questions = [\n",
    "    \"你好\",\n",
    "    \"介绍一下Python编程语言\",\n",
    "    \"什么是机器学习？\"\n",
    "]\n",
    "\n",
    "process = psutil.Process()\n",
    "\n",
    "print(\"Performance Test Results\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Question':<30} {'Time (s)':<12} {'Memory (MB)':<12}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for question in test_questions:\n",
    "    # Get initial memory\n",
    "    mem_before = process.memory_info().rss / (1024 * 1024)\n",
    "    \n",
    "    # Measure time\n",
    "    start_time = time.time()\n",
    "    response = generate_response(question)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Get final memory\n",
    "    mem_after = process.memory_info().rss / (1024 * 1024)\n",
    "    mem_used = mem_after - mem_before\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{question[:28]:<30} {elapsed:<12.2f} {mem_used:<12.2f}\")\n",
    "    print(f\"Response: {response[:50]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Gradio界面启动（可选）\n",
    "\n",
    "如果你想启动完整的Gradio聊天界面，运行下面的单元格。\n",
    "\n",
    "启动后在浏览器中打开: http://127.0.0.1:7860/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Launch Gradio interface\n",
    "# Uncomment the lines below to launch the web interface\n",
    "\n",
    "# import gradio as gr\n",
    "\n",
    "# def gradio_predict(message, history):\n",
    "#     for partial in generate_response(message, history, stream=True):\n",
    "#         yield partial\n",
    "\n",
    "# demo = gr.ChatInterface(\n",
    "#     gradio_predict,\n",
    "#     title=\"Qwen1.5-0.5B-Chat on Orange Pi AI Pro\",\n",
    "#     description=\"基于MindSpore + NPU的Qwen聊天机器人\",\n",
    "#     examples=['你是谁？', '介绍一下Redhat公司', '用Python写一个快速排序']\n",
    "# )\n",
    "\n",
    "# demo.launch()\n",
    "\n",
    "print(\"Gradio launch code is commented out by default.\")\n",
    "print(\"Uncomment the code above to launch the web interface.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试总结\n",
    "\n",
    "如果所有测试都通过，说明你的环境已经正确配置，可以正常运行Qwen聊天机器人。\n",
    "\n",
    "### 预期性能指标:\n",
    "\n",
    "| 指标 | 预期值 |\n",
    "|------|--------|\n",
    "| 模型加载时间 | 1-2 分钟 |\n",
    "| 首次响应时间 | 10-30 秒 |\n",
    "| 后续响应时间 | 5-15 秒 |\n",
    "| 内存占用 | 2-3 GB |\n",
    "\n",
    "### 常见问题:\n",
    "\n",
    "**Q: 首次运行很慢？**\n",
    "A: 需要从Hugging Face下载模型，约1GB数据。下载后会缓存。\n",
    "\n",
    "**Q: 内存不足？**\n",
    "A: 关闭其他程序，或使用更小的模型。\n",
    "\n",
    "**Q: NPU错误？**\n",
    "A: 确保CANN环境正确设置: `source /usr/local/Ascend/ascend-toolkit/set_env.sh`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
