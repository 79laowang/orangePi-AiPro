# Qwen Chatbot with vLLM-Ascend
## High-Performance LLM Inference on Orange Pi AI Pro

åŸºäº vLLM-Ascend çš„é«˜æ€§èƒ½ Qwen èŠå¤©æœºå™¨äººï¼Œæ”¯æŒ Flash Attention å’Œ KV Cache ä¼˜åŒ–ã€‚

## ğŸš€ æ€§èƒ½å¯¹æ¯”

| æ¨ç†å¼•æ“ | tokens/s | ç›¸æ¯”æå‡ |
|---------|----------|---------|
| mindnlp (baseline) | 1.25 | 1x |
| vLLM-Ascend (é¢„æœŸ) | 10-30 | **8-24x** |

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

### ç¡¬ä»¶
- **å¼€å‘æ¿**: Orange Pi AI Pro
- **NPU**: Ascend 310B4
- **å†…å­˜**: è‡³å°‘ 4GB å¯ç”¨ RAM

### è½¯ä»¶
- **æ“ä½œç³»ç»Ÿ**: Ubuntu 22.04.3 LTS (Kernel 5.10.0+)
- **Docker**: 20.10+ (æ¨è) æˆ– Python 3.10+
- **CANN**: 8.1.RC1 æˆ–æ›´é«˜ç‰ˆæœ¬

## ğŸ”§ å®‰è£…æ–¹æ³•

### æ–¹æ³• A: Docker (æ¨è)

Docker å®¹å™¨åŒ…å«æ‰€æœ‰ä¾èµ–ï¼Œå¼€ç®±å³ç”¨ã€‚

```bash
# 1. è¿è¡Œæµ‹è¯•è„šæœ¬
./test_docker.sh

# 2. å¯åŠ¨ vLLM æœåŠ¡å™¨
docker run --rm \
  --name vllm-ascend-server \
  --shm-size=2g \
  --device /dev/davinci0 \
  --device /dev/davinci_manager \
  --device /dev/devmm_svm \
  --device /dev/hisi_hdc \
  -v /usr/local/dcmi:/usr/local/dcmi \
  -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
  -v /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/ \
  -v /root/.cache:/root/.cache \
  -p 8000:8000 \
  -e ASCEND_VISIBLE_DEVICES=0 \
  quay.io/ascend/vllm-ascend:v0.11.0rc1 \
  vllm serve Qwen/Qwen2.5-0.5B-Instruct

# 3. åœ¨å¦ä¸€ä¸ªç»ˆç«¯å¯åŠ¨èŠå¤©åº”ç”¨
python3 app_vllm.py
```

### æ–¹æ³• B: pip å®‰è£…

```bash
# 1. è®¾ç½® CANN ç¯å¢ƒ
source /usr/local/Ascend/ascend-toolkit/set_env.sh

# 2. å®‰è£… vLLM-Ascend
pip install vllm-ascend

# 3. è¿è¡Œæµ‹è¯•
python3 test_vllm.py

# 4. å¯åŠ¨ vLLM æœåŠ¡å™¨
vllm serve Qwen/Qwen2.5-0.5B-Instruct &

# 5. å¯åŠ¨èŠå¤©åº”ç”¨
python3 app_vllm.py
```

## ğŸ“‚ æ–‡ä»¶ç»“æ„

```
vllm-ascend-chat/
â”œâ”€â”€ README.md              # æœ¬æ–‡ä»¶
â”œâ”€â”€ test_docker.sh         # Docker å…¼å®¹æ€§æµ‹è¯•è„šæœ¬
â”œâ”€â”€ test_vllm.py           # Python æµ‹è¯•è„šæœ¬
â”œâ”€â”€ app_vllm.py            # Gradio èŠå¤©åº”ç”¨
â”œâ”€â”€ start_server.sh        # å¯åŠ¨ vLLM æœåŠ¡å™¨è„šæœ¬
â””â”€â”€ docker-compose.yml     # Docker Compose é…ç½®
```

## ğŸ¯ å¿«é€Ÿå¼€å§‹

### 1. å…¼å®¹æ€§æµ‹è¯•

é¦–å…ˆç¡®è®¤ä½ çš„ç³»ç»Ÿæ”¯æŒ vLLM-Ascendï¼š

```bash
chmod +x test_docker.sh
./test_docker.sh
```

æµ‹è¯•åŒ…æ‹¬ï¼š
- Docker å’Œ NPU è®¾å¤‡æ£€æŸ¥
- vLLM-Ascend é•œåƒæ‹‰å–
- æ¨¡å‹åˆå§‹åŒ–æµ‹è¯•
- å•æ¬¡å’Œæ‰¹é‡ç”Ÿæˆæµ‹è¯•

### 2. å¯åŠ¨æœåŠ¡å™¨

```bash
# ä½¿ç”¨å¯åŠ¨è„šæœ¬ (æ¨è)
chmod +x start_server.sh
./start_server.sh

# æˆ–æ‰‹åŠ¨å¯åŠ¨
vllm serve Qwen/Qwen2.5-0.5B-Instruct
```

æœåŠ¡å™¨å°†åœ¨ `http://0.0.0.0:8000` å¯åŠ¨ã€‚

### 3. å¯åŠ¨èŠå¤©åº”ç”¨

```bash
python3 app_vllm.py
```

ç„¶ååœ¨æµè§ˆå™¨æ‰“å¼€: `http://localhost:7860`

## ğŸ” vLLM-Ascend ä¼˜åŠ¿

| ç‰¹æ€§ | mindnlp | vLLM-Ascend |
|-----|---------|-------------|
| Flash Attention | âŒ | âœ… |
| PagedAttention (KV Cache) | âŒ | âœ… |
| è¿ç»­æ‰¹å¤„ç† | âŒ | âœ… |
| OpenAI API å…¼å®¹ | âŒ | âœ… |
| æ¨ç†é€Ÿåº¦ | 1.25 tokens/s | **10-30 tokens/s** |

## ğŸ“Š æ€§èƒ½æµ‹è¯•

è¿è¡Œå®Œæ•´æ€§èƒ½æµ‹è¯•ï¼š

```bash
# Docker æ–¹å¼
./test_docker.sh

# Python æ–¹å¼
python3 test_vllm.py
```

é¢„æœŸè¾“å‡ºï¼š
```
[Test 1/4] Importing vLLM...
âœ… vLLM imported successfully

[Test 2/4] Initializing model...
âœ… Model initialized successfully

[Test 3/4] Testing simple generation...
âœ… Generation successful!
   Speed: 15.23 tokens/s

[Test 4/4] Testing Chinese generation...
âœ… Chinese generation successful!
   Speed: 14.87 tokens/s

ğŸ‰ All tests passed!
```

## âš ï¸ å·²çŸ¥é—®é¢˜

### Ascend 310B4 å…¼å®¹æ€§

**çŠ¶æ€**: å®éªŒæ€§æ”¯æŒ

vLLM-Ascend å®˜æ–¹æ”¯æŒç¡¬ä»¶åˆ—è¡¨ï¼š
- âœ… Atlas A2 ç³»åˆ—
- âœ… Atlas 800I A2
- âœ… Atlas A3 ç³»åˆ—
- âš ï¸ **Ascend 310B4** (Orange Pi AI Pro)

**æ³¨æ„äº‹é¡¹**:
- åŠŸèƒ½åº”è¯¥å¯ä»¥æ­£å¸¸å·¥ä½œ
- æ€§èƒ½å¯èƒ½ä¸å¦‚å®˜æ–¹æ”¯æŒç¡¬ä»¶
- æŸäº›é«˜çº§åŠŸèƒ½å¯èƒ½ä¸æ”¯æŒ

### å†…å­˜é™åˆ¶

Ascend 310B4 NPU å†…å­˜çº¦ 15GBï¼Œå»ºè®®ï¼š
- ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ (â‰¤1B å‚æ•°)
- è®¾ç½® `max_model_len=2048` æˆ–æ›´å°
- é¿å…åŒæ—¶è¿è¡Œå¤šä¸ªå¤§å‹æ¨¡å‹

## ğŸ› ï¸ æ•…éšœæ’é™¤

### é—®é¢˜: Docker å®¹å™¨æ— æ³•è®¿é—® NPU

**ç—‡çŠ¶**:
```
PermissionError: [Errno 13] Permission denied: '/dev/davinci0'
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å°†ç”¨æˆ·æ·»åŠ åˆ° HwHiAiUser ç»„
sudo usermod -aG HwHiAiUser $USER

# é‡æ–°ç™»å½•æˆ–è¿è¡Œ
newgrp HwHiAiUser
```

### é—®é¢˜: æ¨¡å‹ä¸‹è½½å¤±è´¥

**ç—‡çŠ¶**:
```
OSError: Can't load tokenizer for 'Qwen/Qwen2.5-0.5B-Instruct'
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ä½¿ç”¨é•œåƒåŠ é€Ÿ
export VLLM_USE_MODELSCOPE=true

# æˆ–è®¾ç½® HuggingFace é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com
```

### é—®é¢˜: Out of Memory

**ç—‡çŠ¶**:
```
RuntimeError: CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å‡å°æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦
vllm serve Qwen/Qwen2.5-0.5B-Instruct --max-model-len 1024
```

## ğŸ“– ç›¸å…³èµ„æº

- [vLLM-Ascend å®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/projects/ascend/zh-cn/latest/)
- [vLLM-Ascend GitHub](https://github.com/vllm-project/vllm-ascend)
- [Qwen æ¨¡å‹](https://huggingface.co/Qwen)
- [æ˜‡è…¾ç¤¾åŒº](https://www.hiascend.com/)

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ª Apache 2.0 è®¸å¯è¯ã€‚

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

---

**æœ€åæ›´æ–°**: 2026-01-04
